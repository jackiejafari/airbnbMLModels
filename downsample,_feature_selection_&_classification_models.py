# -*- coding: utf-8 -*-
"""Downsample, Feature Selection & Classification Models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wl0rfGcht9yzVloqhmsjD8CHLeOUFvZC
"""

#Import Necessary Packages
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import statsmodels.api as sm
from datetime import datetime
from sklearn.preprocessing import LabelEncoder
from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error,r2_score
import math

#SAGEMAKER ONLY, Need to downgrade pickle to import NLP pickle
!pip3 install pickle5
import pickle5 as pickle
with open("df_NLP.pkl", "rb") as fh:
  df = pickle.load(fh)

type(df)

#load_from_pickle = True
#load_from_pickle = False

#if load_from_pickle:
#    df = pd.read_pickle("df_NLP.pkl")

"""## Additional Data Prep"""

#drop name & price column
df.drop(columns=['name'], inplace=True)
df.drop(columns=['price'], inplace=True)

#One hot encode state
def onehot(dataset, col_name):
    dataset = pd.concat([dataset,pd.get_dummies(dataset[col_name], prefix=col_name, drop_first=True)], axis=1)
    dataset.drop([col_name],axis=1, inplace=True)
    return dataset

df = onehot(df, 'state')
df.head()

"""### Downsampling"""

#view distribution of high and very_high
price_count = df['price_cat'].value_counts()
sns.barplot(x=price_count.index, y=price_count.values)
plt.xlabel('Price Category')
plt.ylabel('Occurrences')
plt.show()

#install downsampling package
!pip install -U imbalanced-learn

#performing downsampling
X = df.drop("price_cat", axis = 1).to_numpy()
y = df["price_cat"].to_numpy()

from imblearn.under_sampling import RandomUnderSampler
obj = RandomUnderSampler()
X_new, y_new = obj.fit_resample(X, y)

print("Number of 0 (low) class intances:", sum(y_new==0))
print("Number of 1 (medium) class instances:", sum(y_new==1))
print("Number of 2 (high) class instances:", sum(y_new==2))

"""## Models on Full Dataset
* Logistic Regression
* KNN
* Naive Bayes
* Random Forest
* Ensemble
"""

#Splitting the data into Training Set and Test Set
X_train, X_test, y_train, y_test = train_test_split(X_new, y_new, test_size=0.3,random_state=0)

#Normalizing the features
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)

"""Logisitic Regression"""

#Fitting Logistic Regression to Training Set
from sklearn.linear_model import LogisticRegression
LRclassifierObj = LogisticRegression()
LRclassifierObj.fit(X_train, y_train)

#Making predictions on the Test Set
y_pred = LRclassifierObj.predict(X_test)

#Print Model Accuracy
accuracy = LRclassifierObj.score(X_test,y_test)
print("Accuracy =", accuracy)
print("Misclassifcation Rate =",1-accuracy)
print()

#Evaluating the predictions using a Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)

"""KNN"""

#Fitting KNN to Training Set
from sklearn.neighbors import KNeighborsClassifier
KNNclassifierObj = KNeighborsClassifier(n_neighbors=6)
KNNclassifierObj.fit(X_train, y_train)

#Making predictions on the Test Set
y_pred = KNNclassifierObj.predict(X_test)

#Print Model Accuracy
accuracy = KNNclassifierObj.score(X_test, y_test)
print("Accuracy =", accuracy)
print("Misclassifcation Rate =",1-accuracy)
print()

#Evaluating the predictions using a Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)

"""Naive Bayes"""

from sklearn.naive_bayes import GaussianNB
NBclassifierObj = GaussianNB()
accuracy = NBclassifierObj.fit(X_train, y_train).score(X_test, y_test)

#Making predictions on the Test Set
y_pred = NBclassifierObj.predict(X_test)

#Print Model Accuracy
print("Accuracy =", accuracy)
print("Misclassifcation Rate =",1-accuracy)
print()

#Evaluating the predictions using a Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)

"""Random Forest"""

from sklearn.ensemble import RandomForestClassifier
RFCclassifierObj = RandomForestClassifier(n_estimators=100,criterion='entropy')
accuracy = RFCclassifierObj.fit(X_train, y_train).score(X_test,y_test)

#Making predictions on the Test Set
y_pred = RFCclassifierObj.predict(X_test)

#Print Model Accuracy
print("Accuracy =", accuracy)
print("Misclassifcation Rate =",1-accuracy)
print()

#Evaluating the predictions using a Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)

"""Ensemble"""

pred_ensemble = []
for instance in (X_test):
    #Convert Instance to a 2D-Array
    rows = instance.shape[0]
    instance = instance.reshape(1,rows)

    #Make Predictions to 4 Models
    Logistic_y = LRclassifierObj.predict(instance)
    KNN_y = KNNclassifierObj.predict(instance)
    NB_y = NBclassifierObj.predict(instance)
    RandomForest_y = RFCclassifierObj.predict(instance)

    #append 4 predictions
    pred_4models = Logistic_y.item(), KNN_y.item(), NB_y.item(), RandomForest_y.item()

    #Identify majority prediction
    pred_majority = max(pred_4models, key = pred_4models.count)
    pred_ensemble.append(pred_majority)

#manually calculating accuracy
def accuracy(confusion_matrix):
    diagonal_sum = confusion_matrix.trace()
    sum_of_all_elements = confusion_matrix.sum()
    return diagonal_sum / sum_of_all_elements

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, pred_ensemble)

print("Accuracy =", accuracy(cm))
print("Misclassifcation Rate =",1-accuracy(cm))
print()
print(cm)

"""## Feature Selection with Backward Elimination"""

#Backward Elimination Loop
#Split Raw data into X and y
rows = X_new.shape[0]
columns = X_new.shape[1]

X_new = np.append(arr=np.ones((rows,1)).astype(int), values=X_new, axis=1)
X_new = X_new.astype('float64')

def backwardElimination(x, sl):
    numVars = len(x[0])
    for i in range(0, numVars):
        obj_OLS = sm.OLS(y_new, x).fit()
        maxVar = max(obj_OLS.pvalues).astype(float)
        if maxVar > sl:
            for j in range(0, numVars - i):
                if (obj_OLS.pvalues[j].astype(float) == maxVar):
                    x = np.delete(x, j, 1)
    obj_OLS.summary()
    return x

SL = 0.01
X_sig = X_new[:, 0:columns+1]
X_Modeled = backwardElimination(X_sig, SL)

print(X_Modeled.shape)

"""## Models on feature selection data
* Logistic Regression
* KNN
* Naive Bayes
* Random Forest
* Ensemble
"""

#Split Backward Elimination into train test
#Splitting the data into Training Set and Test Set
X_sig_train, X_sig_test, y_sig_train, y_sig_test = train_test_split(X_Modeled, y_new, test_size=0.3,random_state=0)

#Normalizing the features
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_sig_train = sc_X.fit_transform(X_sig_train)
X_sig_test = sc_X.transform(X_sig_test)

"""Logistic Regression"""

#Fitting Logistic Regression to Training Set
from sklearn.linear_model import LogisticRegression
LRclassifierObj = LogisticRegression()
LRclassifierObj.fit(X_sig_train, y_sig_train)

#Making predictions on the Test Set
y_pred = LRclassifierObj.predict(X_sig_test)

#Print Model Accuracy
accuracy = LRclassifierObj.score(X_sig_test,y_sig_test)
print("Accuracy =", accuracy)
print("Misclassifcation Rate =",1-accuracy)
print()

#Evaluating the predictions using a Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_sig_test, y_pred)
print(cm)

"""KNN"""

#Fitting KNN to Training Set
from sklearn.neighbors import KNeighborsClassifier
KNNclassifierObj = KNeighborsClassifier(n_neighbors=6)
KNNclassifierObj.fit(X_sig_train, y_sig_train)

#Making predictions on the Test Set
y_pred = KNNclassifierObj.predict(X_sig_test)

#Print Model Accuracy
accuracy = KNNclassifierObj.score(X_sig_test,y_sig_test)
print("Accuracy =", accuracy)
print("Misclassifcation Rate =",1-accuracy)
print()

#Evaluating the predictions using a Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_sig_test, y_pred)
print(cm)

from sklearn.naive_bayes import GaussianNB
NBclassifierObj = GaussianNB()
accuracy = NBclassifierObj.fit(X_sig_train, y_sig_train).score(X_sig_test,y_sig_test)

#Making predictions on the Test Set
y_pred = NBclassifierObj.predict(X_sig_test)

#Print Model Accuracy
print("Accuracy =", accuracy)
print("Misclassifcation Rate =",1-accuracy)
print()

#Evaluating the predictions using a Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_sig_test, y_pred)
print(cm)

#This Model takes forever to run

#from sklearn.svm import SVC
#SVCclassifierObj = SVC(kernel = 'linear')
#accuracy = SVCclassifierObj.fit(X_sig_train, y_sig_train).score(X_sig_test,y_sig_test)

#Making predictions on the Test Set
#y_pred = SVCclassifierObj.predict(X_sig_test)

#Print Model Accuracy
#print("Accuracy =", accuracy)
#print("Misclassifcation Rate =",1-accuracy)
#print()

#Evaluating the predictions using a Confusion Matrix
#from sklearn.metrics import confusion_matrix
#cm = confusion_matrix(y_sig_test, y_pred)
#print(cm)

from sklearn.ensemble import RandomForestClassifier
RFCclassifierObj = RandomForestClassifier(n_estimators=100,criterion='entropy')
accuracy = RFCclassifierObj.fit(X_sig_train, y_sig_train).score(X_sig_test,y_sig_test)

#Making predictions on the Test Set
y_pred = RFCclassifierObj.predict(X_sig_test)

#Print Model Accuracy
print("Accuracy =", accuracy)
print("Misclassifcation Rate =",1-accuracy)
print()

#Evaluating the predictions using a Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_sig_test, y_pred)
print(cm)

pred_ensemble = []
for instance in (X_sig_test):
    #Convert Instance to a 2D-Array
    rows = instance.shape[0]
    instance = instance.reshape(1,rows)

    #Make Predictions to 4 Models
    Logistic_y = LRclassifierObj.predict(instance)
    KNN_y = KNNclassifierObj.predict(instance)
    NB_y = NBclassifierObj.predict(instance)
    RandomForest_y = RFCclassifierObj.predict(instance)

    #append 4 predictions
    pred_4models = Logistic_y.item(), KNN_y.item(), NB_y.item(), RandomForest_y.item()

    #Identify majority prediction
    pred_majority = max(pred_4models, key = pred_4models.count)
    pred_ensemble.append(pred_majority)

#manually calculating accuracy
def accuracy(confusion_matrix):
    diagonal_sum = confusion_matrix.trace()
    sum_of_all_elements = confusion_matrix.sum()
    return diagonal_sum / sum_of_all_elements

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_sig_test, pred_ensemble)
print(cm)
print()
print("Accuracy =", accuracy(cm))
print("Misclassifcation Rate =",1-accuracy(cm))

"""## Feature Extraction
* PCA
* LDA
* Kernal PCA
"""

#Splitting the data into Training Set and Test Set
X_train, X_test, y_train, y_test = train_test_split(X_new, y_new, test_size=0.3,random_state=0)

#Normalizing the features
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)

#Applying PCA
from sklearn.decomposition import PCA
pcaObj = PCA(n_components=2)
X_train = pcaObj.fit_transform(X_train)
X_test = pcaObj.transform(X_test)
components_variance = pcaObj.explained_variance_ratio_

#Fitting Logistic Regression to Training Set
from sklearn.linear_model import LogisticRegression
classifierObj = LogisticRegression()
classifierObj.fit(X_train, y_train)

#Making predictions on the Test Set
y_pred = classifierObj.predict(X_test)

#Print Model Accuracy
print(classifierObj.score(X_test,y_test))

#Evaluating the predictions using a Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)

#Splitting the data into Training Set and Test Set
X_train, X_test, y_train, y_test = train_test_split(X_new, y_new, test_size=0.3,random_state=0)

#Normalizing the features
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)

#Applying LDA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
ldaObj = LDA(n_components=2)
X_train = ldaObj.fit_transform(X_train,y_train) #Have to provide y information with LDA
X_test = ldaObj.transform(X_test)

#Fitting Logistic Regression to Training Set
from sklearn.linear_model import LogisticRegression
classifierObj = LogisticRegression()
classifierObj.fit(X_train, y_train)

#Making predictions on the Test Set
y_pred = classifierObj.predict(X_test)

#Print Model Accuracy
print(classifierObj.score(X_test,y_test))

#Evaluating the predictions using a Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)

#Splitting the data into Training Set and Test Set
X_train, X_test, y_train, y_test = train_test_split(X_new, y_new, test_size=0.3,random_state=0)

#Normalizing the features
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)

#kernel PCA
from sklearn.decomposition import KernelPCA
kernelPCAObj = KernelPCA(n_components=1, kernel='linear')
X_train = kernelPCAObj.fit_transform(X_train)
X_test = kernelPCAObj.transform(X_test)

#Fitting Logistic Regression to Training Set
from sklearn.linear_model import LogisticRegression
classifierObj = LogisticRegression()
classifierObj.fit(X_train, y_train)

#Making predictions on the Test Set
y_pred = classifierObj.predict(X_test)

#Print Model Accuracy
print(classifierObj.score(X_test,y_test))

#Evaluating the predictions using a Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)

