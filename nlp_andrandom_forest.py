# -*- coding: utf-8 -*-
"""NLP andRandom Forest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zl_jyiod7fGoMlQNkguEIi0eQ_al-N7j

# Regression by Parts
Classify data, use appropriate regression by  classification ouput

#### Import all used libraries
"""

#Import Necessary Packages
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import warnings
import pickle
from datetime import datetime
from sklearn.preprocessing import LabelEncoder
from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error,r2_score
from sklearn.preprocessing import StandardScaler
import math
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors import KNeighborsRegressor
from sklearn import metrics
from sklearn import preprocessing
import statsmodels.api as sm
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix

# optional for showing execution time (install in anaconda and restart jupyter)
#pip install jupyter_contrib_nbextensions
#jupyter contrib nbextension install --user
#jupyter nbextension enable execute_time/ExecuteTime

# NLP
from collections import Counter
import operator
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
import re
import string
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import MultiLabelBinarizer

"""# import data and filter"""

load_from_pickle = True
#load_from_pickle = False

def onehot(dataset, col_name):
    dataset = pd.concat([dataset,pd.get_dummies(dataset[col_name], prefix=col_name, drop_first=True)], axis=1)
    dataset.drop([col_name],axis=1, inplace=True)
    return dataset

if load_from_pickle:
    df = pd.read_pickle("df.pkl")
else:
    # Import data set
    # Replace missing values with NaN while loading the data
    df = pd.read_csv('AB_US_2020.csv', na_values = ['?', '&'], low_memory=False)

    # replace missing values
    df['reviews_per_month'] = df['reviews_per_month'].fillna(0)
    df['neighbourhood'] = df['neighbourhood'].fillna("other_neighbourhood")
    df['neighbourhood_group'].fillna('other',inplace=True)

    # filter out outliers (over 3 sigma out)
    sigma3 = 0.997
    df = df[df['price'] < df['price'].quantile(sigma3)]
    df = df[df['minimum_nights'] < df['minimum_nights'].quantile(sigma3)]
    df = df[df['reviews_per_month'] < df['reviews_per_month'].quantile(sigma3)]
    df = df[df['calculated_host_listings_count'] < df['calculated_host_listings_count'].quantile(sigma3)]
    df = df[df['number_of_reviews'] < df['number_of_reviews'].quantile(sigma3)]

    # convert 'last review date' to 'time since last review'
    last_day = pd.to_datetime('31/12/2021')
    delta = (last_day - pd.to_datetime(df.last_review)).astype('timedelta64[D]')
    df['days_since_last_review'] = delta
    # replace null values with max value (arbitrary)
    max_time = df['days_since_last_review'].max()
    df["days_since_last_review"].fillna(max_time, inplace = True)

    # drop columns
    df.drop(columns=['last_review'], inplace=True)
    df.drop(columns=['id'], inplace=True)
    df.drop(columns=['host_name'], inplace=True)
    df.drop(columns=['host_id'], inplace=True)
    df.drop(columns=['neighbourhood'], inplace=True)
    df.drop(columns=['neighbourhood_group'], inplace=True)
    df.drop(columns=['latitude'], inplace=True)
    df.drop(columns=['longitude'], inplace=True)

    # rename 'calculated_host_listings_count' to 'host_listings_count'
    df['host_listings_count'] = df['calculated_host_listings_count']
    df.drop(columns=['calculated_host_listings_count'], inplace=True)

    # one hot encoding
    df = onehot(df, 'room_type')

    # split price into categories
    '''
    conditions = [
        (df['price'] < 100),
        (df['price'] >= 100) & (df['price'] < 250),
        (df['price'] >= 250) & (df['price'] < 600),
        (df['price'] >= 600) ]
    '''
    conditions = [
        (df['price'] < 100),
        (df['price'] >= 100) & (df['price'] < 250),
        (df['price'] >= 250) ]
    # label for each condition
    labels = range(len(conditions))
    # combine new column
    df['price_cat'] = np.select(conditions, labels)

    # only replace column if it hasn't already been done
    if 'city' in df.columns:
        conditions = [
            (df['city'] == 'New York City'),
            (df['city'] == 'Columbus'),
            (df['city'] == 'Hawaii'),
            (df['city'] == 'Asheville'),
            (df['city'] == 'Jersey City'),
            (df['city'] == 'Washington D.C.'),
            (df['city'] == 'Clark County'),
            (df['city'] == 'Rhode Island'),
            (df['city'] == 'Portland'),
            (df['city'] == 'Austin'),
            (df['city'] == 'Broward County'),
            (df['city'] == 'Seattle'),
            (df['city'] == 'Twin Cities MSA'),
            (df['city'] == 'New Orleans'),
            (df['city'] == 'Chicago'),
            (df['city'] == 'Nashville'),
            (df['city'] == 'Denver'),
            (df['city'] == 'Cambridge') | (df['city'] == 'Boston') | (df['city'] == 'Salem'),
            (df['city'] == 'Los Angeles') | (df['city'] == 'Oakland') | (df['city'] == 'San Diego') |
                        (df['city'] == 'San Francisco') | (df['city'] == 'Santa Cruz County') |
                        (df['city'] == 'Pacific Grove')| (df['city'] == 'San Clara Country') |
                        (df['city'] == 'San Mateo County')
            ]

        state_values = ['NY', 'OH', 'HI', 'NC', 'NJ',
                        'DC', 'NV', 'RI', 'OR', 'TX',
                        'FL', 'WA', 'MN', 'LA', 'IL',
                        'TN', 'CO', 'MA', 'CA']

        df['state'] = np.select(conditions, state_values)
        df.drop(columns=['city'], inplace=True)
    else:
        warnings.warn('df already transformed')

    # create sub-df for each state
    df_CA = df[df['state'] == 'CA']
    df_NY = df[df['state'] == 'NY']
    df_HI = df[df['state'] == 'HI']
    df_FL = df[df['state'] == 'FL']
    df_TX = df[df['state'] == 'TX']
    df_NV = df[df['state'] == 'NV']
    df_DC = df[df['state'] == 'DC']
    df_WA = df[df['state'] == 'WA']
    df_LA = df[df['state'] == 'LA']
    df_MN = df[df['state'] == 'MN']
    df_IL = df[df['state'] == 'IL']
    df_TN = df[df['state'] == 'TN']
    df_MA = df[df['state'] == 'MA']
    df_OR = df[df['state'] == 'OR']
    df_CO = df[df['state'] == 'CO']
    df_RI = df[df['state'] == 'RI']
    df_NJ = df[df['state'] == 'NJ']
    df_NC = df[df['state'] == 'NC']
    df_OH = df[df['state'] == 'OH']

    df_states = [df_NY, df_OH, df_HI, df_NC, df_NJ, df_DC,
                 df_NV, df_RI, df_OR, df_TX, df_FL, df_WA,
                 df_MN, df_LA, df_IL, df_TN, df_CO, df_MA,
                 df_CA]

    # pickle df
    df.to_pickle("df.pkl")

"""### Prepare the 'name' title for NLP by splitting words and stem/lemmatizing them"""

load_from_pickle = True
#load_from_pickle = False    # runtime: 24s

def set_to_dict(s):
    return dict.fromkeys(s, 1)

if load_from_pickle:
    df_nlp = pd.read_pickle("df_NLP.pkl")
else:
    df_nlp = df.copy()
    df_nlp['word_length'] = df_nlp['name'].apply(lambda x : len(str(x).split()))

    # split the name column into a list of words
    df_nlp['name'] = df_nlp['name'].str.lower()
    df_nlp['name'] = df_nlp['name'].str.split()

    # Read in and strip the stopwords
    stop_words = list(stopwords.words('english'))
    stop_words.extend(['+', '&', '!', '-'])
    max_word_length = 12

    def remove_stopwords(word_list):
        res = []
        if isinstance(word_list, list):
            for w in word_list:
                if w not in stop_words and len(w) <= max_word_length:
                    w= ''.join(ch for ch in w if w.isalpha())
                    res.append(w)
        return res
    df_nlp['name'] = df_nlp['name'].apply(remove_stopwords)

    # Lemmetize words
    lemmatizer = WordNetLemmatizer()
    def get_lemma(words):
        return [lemmatizer.lemmatize(word) for word in words]
    df_nlp['name'] = df_nlp['name'].apply(get_lemma)

    # Stem words
    stemmer = PorterStemmer()
    def stem_list(words):
        return set([stemmer.stem(word) for word in words])
    df_nlp['name'] = df_nlp['name'].apply(stem_list)

    # build set of unique words
    uniqueWords = set()
    df_nlp['name'].apply(uniqueWords.update)

    # convert to a default dictionary so that we can tally which words are most popular
    df_nlp['name'] = df_nlp['name'].apply(set_to_dict)

    # while we are here, lets also grab an effective word length (excluding the BS)
    df_nlp['word_length_eff'] = df_nlp['name'].apply(len)

    # pickle df_nlp at this point
    df_nlp.to_pickle("df_NLP.pkl")

"""### Capture most popularly used words accross all listings"""

load_from_pickle = True
#load_from_pickle = False   # 8m 37s

most_popular_words = {}
def combine_dict(d):
    global most_popular_words
    most_popular_words = {x: most_popular_words.get(x, 0) + d.get(x, 0) for x in set(most_popular_words).union(d)}

if load_from_pickle:
    with open("most_popular_words.pkl", 'rb') as handle:
        most_popular_words = pickle.load(handle)
else:
    df_nlp['name'].apply(combine_dict)

    # sort by value
    most_popular_words = dict(sorted(most_popular_words.items(), key=lambda item: item[1], reverse=True))

    with open('most_popular_words.pkl', 'wb') as fn:
        pickle.dump(most_popular_words, fn, protocol=pickle.HIGHEST_PROTOCOL)

# only keep words if they are used more than x_popular times (arbitrary)
# try to keep #words above 500 for best results and below 1000 for best performance (time)
x_popular = 100
filtered_most_popular_words = {key:val for key, val in most_popular_words.items() if val >= x_popular}
filtered_most_popular_words.pop('', None)
print('#words w/ >'+str(x_popular)+' occurances: '+str(len(filtered_most_popular_words)))

# rearrange columns
df_nlp["price_$"] = df_nlp["price"]
df_nlp["name"] = df_nlp["name"].apply(list)

# filter name_cntr to only include words in filtered_most_popular_words
list_most_popular_words = list(filtered_most_popular_words)
def filter_words(words):
    res = []
    for word in words:
        if word in list_most_popular_words:
            res.append(word)
    return res
df_nlp['name'] = df_nlp['name'].apply(filter_words)

# rename overlapping columns
df_nlp['price_'] = df_nlp['price']
df_nlp['state_'] = df_nlp['state']
df_nlp['name_'] = df_nlp['name']
df_nlp.drop(columns=['price'], inplace=True)
df_nlp.drop(columns=['state'], inplace=True)
df_nlp.drop(columns=['name'], inplace=True)

# binarize the words into columns
mlb = MultiLabelBinarizer(sparse_output=True)
df_nlp = df_nlp.join(pd.DataFrame.sparse.from_spmatrix(
                mlb.fit_transform(df_nlp.pop('name_')),
                index=df.index,
                columns=mlb.classes_))

# pickle df_nlp as a second checkpoint
df_nlp.to_pickle("df_NLP2.pkl")

"""# Random forest using frequent words, done for each state"""

load_from_pickle = True
#load_from_pickle = False     # execution time: 17m25s (1GB pkl produced!!!)

df2 = pd.read_pickle("df_NLP2.pkl")

def misclassification_rate(cm):
    misclassification_rate = (np.sum(cm) - np.trace(cm)) / np.sum(cm)
    return misclassification_rate

classifiers_by_state = []
if load_from_pickle:
    with open("random_forest_model.pkl", 'rb') as fn:
        classifiers_by_state = pickle.load(fn)
else:
    for n in range(len(state_values)):
        print(state_values[n])
        df3 = df2[df2['state_'] == state_values[n]].copy()
        df3.drop(['price_$'],axis=1, inplace=True)
        df3.drop(['price_'],axis=1, inplace=True)
        df3.drop(['state_'],axis=1, inplace=True)
        df3['reviews_per_month'] = df3['reviews_per_month']*1000
        df3 = df3.astype('int32')
        df3 = df3.reset_index()

        for col in df3.columns[12:]:
            if sum(df3[col]) == 0:
                df3.drop([col],axis=1, inplace=True)
        print(len(df3.columns))

        X = df3.drop(['price_cat'],axis=1)
        y = df3['price_cat']

        # Split into training and test
        X_train, X_test, y_train, y_test = train_test_split(X,y , test_size = 0.3, random_state=0)

        # scale values
        sc_X = StandardScaler()
        X_train = sc_X.fit_transform(X_train)
        X_test = sc_X.transform(X_test)

        n_est = 100
        #Fitting Classifier to Training Set
        classifierObjRF = RandomForestClassifier(n_estimators=n_est, criterion='entropy')
        classifierObjRF.fit(X_train,y_train)

        #Making predictions on the Test Set
        y_pred = classifierObjRF.predict(X_test)

        #Evaluating the predictions using a Confusion Matrix
        cm = confusion_matrix(y_test, y_pred)

        data = [classifierObjRF,
                n_est,
                misclassification_rate(cm),
                X_train,
                y_train,
                X_test,
                y_test,
                y_pred
               ]
        classifiers_by_state.append(data)

    with open('random_forest_model.pkl', 'wb') as fn:
        pickle.dump(classifiers_by_state, fn, protocol=pickle.HIGHEST_PROTOCOL)

print('state \t acc')
for n in range(len(classifiers_by_state)):
    print(state_values[n],'\t', round(1-classifiers_by_state[n][2],3))

# to look into the 2nd classification, start here...

with open("random_forest_model.pkl", 'rb') as fn:
    classifiers_by_state = pickle.load(fn)

print('state \t acc')
for n in range(len(classifiers_by_state)):
    print(state_values[n],'\t', round(1-classifiers_by_state[n][2],3))

